---
title: "Homework 2"
author: "Christina Ridlen"
date: "`r Sys.Date()`"
output: md_document
---

```{r libraries, include = FALSE}
library(tidyverse)
library(modelr)
library(rsample)
library(mosaic)
library(caret)
library(parallel)
library(foreach)
library(timeDate)
library(lubridate, warn.conflicts = TRUE)
library(pROC)
library(knitr)
```

```{r data, include = FALSE}
setwd("C:/")
capmetroUT <- read.csv("Users\\tinar\\OneDrive\\Desktop\\repos\\ECO395M\\data\\capmetro_UT.csv")
capmetroUT <- mutate(capmetroUT,
               day_of_week = factor(day_of_week,
                 levels=c("Mon", "Tue", "Wed","Thu", "Fri", "Sat", "Sun")),
               month = factor(month,
                 levels=c("Sep", "Oct","Nov")))
plotdata <- capmetroUT %>%
  group_by(month, day_of_week, hour_of_day) %>%
  summarize(avg_board = mean(boarding))
```

# Problem 1

## Average Boarding

```{r, echo = FALSE }

ggplot(plotdata) +
  geom_line(aes(hour_of_day, avg_board, col = month)) +
  facet_wrap(~day_of_week) +
  labs(title = "Average Boarding Trends for UT Cap Metro Service Sep-Nov 2018",
       x = "Hour of Day",
       y = "Average Boardings")
  
```

Average boardings per hour are very similar during workdays. In September, the average ridership looks significantly lower than in the other months. It may be because school has just started and students are trying to get a good start to their weeks on Mondays, so they leave with enough time to walk instead of taking the bus. In November, the average number of boardings is not much different than the other months, but falls noticeably in the later days of the week. By now the semester is so busy that students can barely leave the house on time at the beginning of the week, but since the weather is nice and the weekend is approaching soon they prefer to walk to class.

## Boardings and Temperature

```{r data wrangling, include = FALSE}
capmetroUT %>%
  ggplot() +
  geom_point(aes(x = temperature, y = boarding, col = weekend)) + 
  facet_wrap(~hour_of_day) +
  labs(title = "Average Boarding by Temperature each Hour",
       x = 'Temperature',
       y = "Average number of boardings")

```

The data seem to be clustered at the right side of the graph, which is because the interquartile range for `temperature` is about 60 degrees to 80 degrees with a median of 73 degrees. Average ridership itself does not seem to vary that much with temperature; average boardings seem to be evenly distributed across temperatures.

# Problem 2

## Linear Model

```{r saratoga_baseline, include = FALSE}
data("SaratogaHouses")
saratoga_split = initial_split(SaratogaHouses, prop = 0.8)
saratoga_train = training(saratoga_split)
saratoga_test = testing(saratoga_split)

# Baseline RMSE
lm2 = lm(price ~ . - pctCollege - sewer - waterfront - landValue - newConstruction, data=saratoga_train)
rmselm2 <- rmse(lm2, saratoga_test)

```

```{r saratoga_linear, echo = FALSE}
# My linear model

saratoga_lm = lm(price ~ . -pctCollege - fireplaces - waterfront + bedrooms*bathrooms + landValue*lotSize,  data = saratoga_train)
rmse_lm <- rmse(saratoga_lm, saratoga_test)
rmselm2 - rmse_lm
```

My linear model regresses price on all variables excluding `pctCollege` `fireplaces` `waterfront` and includes interactions on `landValue` and `lotSize` , as well as on `bathrooms` and `bedrooms.` This model performs better than the "medium" model from class.

## KNN Model

```{r standardize, include = FALSE}
saratoga_std <- SaratogaHouses %>% mutate_at(c('lotSize', 'age', 'landValue', 'livingArea', 'bedrooms', 'bathrooms', 'rooms'), ~(scale(.) %>% as.vector))

# new train test split
saratoga_std_split = initial_split(saratoga_std, prop = 0.8)
saratoga_std_train = training(saratoga_std_split)
saratoga_std_test = testing(saratoga_std_split)

```

```{r knn_model, include = FALSE}
knn10 = knnreg(price ~ . -pctCollege - fireplaces - waterfront, data=saratoga_std_train, k=10)
rmse(knn10, saratoga_std_test)
```

After trying different variations of the model, a regression with all variables excluding `pctCollege`, `fireplaces` and `waterfront` minimized the RMSE.

## K-Fold Cross Validation

### KNN Model

```{r K_fold, include = FALSE}
K_folds = 10
saratoga_std_folds = crossv_kfold(saratoga_std, k = K_folds)
# Perform over k-grid
k_grid = c(2, 4, 6, 8, 10, 15, 20, 25, 30, 35, 40, 45,
           50, 60, 70, 80, 90, 100, 125, 150, 175, 200, 250, 300)

cv_grid = foreach(k = k_grid, .combine='rbind') %dopar% {
  models = map(saratoga_std_folds$train, ~knnreg(price ~  . -pctCollege - fireplaces - waterfront, data=., k=k, use.all=FALSE))
  errs = map2_dbl(models, saratoga_std_folds$test, modelr::rmse)
  c(k=k, err = mean(errs), std_err = sd(errs)/sqrt(K_folds))
} %>% as.data.frame
```

```{r plot_kfold_knn, echo = FALSE}
ggplot(cv_grid) + 
  geom_point(aes(x=k, y=err)) + 
  geom_errorbar(aes(x=k, ymin = err-std_err, ymax = err+std_err)) + 
  scale_x_log10() + 
  labs(x = "K",
       y = "Average Error")
# K = 15 has lowest error

```

Using K-fold Cross Validation, we confirm that K = 15 is the best K for the model, choosing with the "1SE" rule, which results in an RMSE of 61010.

### Linear Model

Performing K-fold cross validation with the linear model, we calculate the approximated lowest RMSE:

```{r kfold_lm, echo = FALSE}
# create folds on nonstandardized variables
saratoga_folds = crossv_kfold(SaratogaHouses, k = K_folds)
#use caret package to perform k-fold cross validation
ctrl = trainControl(method = "cv", number = 10)
model <- train(price ~ . -pctCollege - fireplaces - waterfront + bedrooms*bathrooms + landValue*lotSize,  data = saratoga_train, method = "lm", trControl = ctrl)
print(model)
```

The linear regression model produces a lower RMSE than the KNN model, so in this case, a more general parametric model might be more useful for this data in particular. A highlight of using a linear regression model is that we can use field-specific knowledge to choose our features, rather than letting KNN regression do the work for us.

# Problem 3

```{r data, include = FALSE}
german_credit <- read.csv("Users\\tinar\\OneDrive\\Desktop\\repos\\ECO395M\\data\\german_credit.csv")


ggplot(german_credit) +
  geom_col(aes(x = history, y = Default))


def_prob <- german_credit %>%
  group_by(history) %>%
  summarize(prob = sum(Default)/n())



```

```{r plot, echo = FALSE}
def_prob %>%
  ggplot() +
  geom_col(aes(x = history, y = prob, fill = history)) + 
  labs(title = "Probability of Default by Credit History",
       x = "Credit History",
       y = "Probability")

# GLM regression
logit_default = glm(Default ~ duration + amount + installment + age + history + purpose + foreign, data = german_credit, family ='binomial')
coef(logit_default) %>% round(2)

```

Ironically, the probability of default increases with credit history score. Looking at the data more closely, this unexpected result comes from poor sampling:

```{r counts, echo = FALSE}
german_credit %>%
  mutate(History = history) %>%
  group_by(History)%>%
  summarize("Number of Defaults" = sum(Default == 1), N = n())
```

Obviously, the probability of default when `history == "good"` is high because out of only 89 people sampled in that category, 53 of them had defaulted on their loans. The bank should take a new random sample, making sure to have an equal number of borrowers from each respective credit history category.

# Problem 4

```{r data, include = FALSE}
#logit regression
hotelsdev <- read.csv("C:\\Users\\tinar\\OneDrive\\Desktop\\repos\\ECO395M\\data\\hotels_dev.csv")

hotelsdev <- hotelsdev %>%
  mutate(arrival_date = as.Date(arrival_date, format = "%Y-%m-%d"), is_holiday = ifelse(arrival_date %in% c(as.Date("2016-01-01"),
as.Date("2017-01-01"),
as.Date(Easter(2016:2017)),
as.Date(USGoodFriday(2016:2017)),
as.Date(USMemorialDay(2016:2017)),
as.Date(USIndependenceDay(2015:2017)),
as.Date(USLaborDay(2015:2016)),
as.Date(USThanksgivingDay(2015:2016)),
as.Date(USChristmasDay(2015:2016))), "yes" = 1, "no" = 0), is_summer = ifelse(between(arrival_date, as.Date("2015-07-01"), as.Date("2015-09-20")) | between(arrival_date, as.Date("2016-06-21"), as.Date("2016-09-20")) | between(arrival_date, as.Date("2017-06-21"), as.Date("2017-08-31")), 1, 0))

# Baseline 1
# train - test split

hotelsdev_split = initial_split(hotelsdev, prop = 0.8)
hotelsdev_train = training(hotelsdev_split)
hotelsdev_test = testing(hotelsdev_split)

b1 <- glm(children ~ market_segment + adults + customer_type + is_repeated_guest, family = 'binomial', data = hotelsdev_train)

predicted1 <- predict(b1, hotelsdev_test, type = 'response')
yhat1<- ifelse(predicted1 > .5, 1, 0)
confusion1= table(y = hotelsdev_test$children, 
                  yhat = yhat1)
confusion1
sum(diag(confusion1))/sum(confusion1)




#Baseline 2 
baseline_2 <- glm(children ~ . - arrival_date - is_summer - is_holiday, data = hotelsdev_train, family='binomial')

predicted2 <- predict(baseline_2, hotelsdev_test, type = 'response')
yhat2<- ifelse(predicted2 > 0.5, 1, 0)
confusion2 = table(y = hotelsdev_test$children,
                   yhat = yhat2)
confusion2
b2ac <- sum(diag(confusion2))/sum(confusion2)


# Baseline 3

lm_best =  glm(children ~ . - arrival_date + is_summer*hotel + is_holiday*hotel + is_summer*stays_in_week_nights, family = 'binomial', data = hotelsdev_train)

             
```

```{r confusion_dev, include = FALSE}

predicted <- predict(lm_best, hotelsdev_test, type = 'response')
yhat <- ifelse(predicted > 0.5, 1, 0)
confusion = table(y = hotelsdev_test$children, 
                  yhat = yhat)
confusion
best_ac <- sum(diag(confusion))/sum(confusion)
b2ac - best_ac
```

```{r compare_acc, echo = FALSE}
baseline_2 <- glm(children ~ . - arrival_date - is_summer - is_holiday, data = hotelsdev_train, family='binomial')

predicted2 <- predict(baseline_2, hotelsdev_test, type = 'response')
yhat2<- ifelse(predicted2 > 0.5, 1, 0)
confusion2 = table(y = hotelsdev_test$children,
                   yhat = yhat2)
confusion2
b2ac <- sum(diag(confusion2))/sum(confusion2)

lm_best <- glm(children ~ . - arrival_date + is_summer*hotel + is_holiday*hotel + is_summer*stays_in_week_nights, family = 'binomial', data = hotelsdev_train)

predicted <- predict(lm_best, hotelsdev_test, type = 'response')
yhat <- ifelse(predicted > 0.5, 1, 0)
confusion = table(y = hotelsdev_test$children, 
                  yhat = yhat)
confusion
best_ac <- sum(diag(confusion))/sum(confusion)
b2ac - best_ac
```

The out of sample accuracy is almost 94% for the linear model specified, which is slightly higher than the baseline 2 model, where `is_summer` and `is_holiday` are features I generated from `arrival_date`.

### Model Validation: Step 1

```{r hotels_val, echo = FALSE}

hotels_val <- read.csv("C:\\Users\\tinar\\OneDrive\\Desktop\\repos\\ECO395M\\data\\hotels_val.csv")

hotels_val <- hotels_val %>%
  mutate(arrival_date = as.Date(arrival_date, format = "%Y-%m-%d"), is_holiday = ifelse(arrival_date %in% c(as.Date("2016-01-01"),
as.Date("2017-01-01"),
as.Date(Easter(2016:2017)),
as.Date(USGoodFriday(2016:2017)),
as.Date(USMemorialDay(2016:2017)),
as.Date(USIndependenceDay(2015:2017)),
as.Date(USLaborDay(2015:2016)),
as.Date(USThanksgivingDay(2015:2016)),
as.Date(USChristmasDay(2015:2016))), "yes" = 1, "no" = 0), is_summer = ifelse(between(arrival_date, as.Date("2015-07-01"), as.Date("2015-09-20")) | between(arrival_date, as.Date("2016-06-21"), as.Date("2016-09-20")) | between(arrival_date, as.Date("2017-06-21"), as.Date("2017-08-31")), 1, 0))


val_split = initial_split(hotels_val, prop = 0.8)
val_train = training(val_split)
val_test = testing(val_split)


# Baseline 3
lm_best= glm(children ~ . - arrival_date + is_summer*hotel + is_holiday*hotel + is_summer*stays_in_week_nights, family = 'binomial', data = val_train)


#ROC Curve
predicted_val <- predict(lm_best, val_test, type = 'response')
y_hat_val = ifelse(predicted_val > 0.5, 1, 0)
confusion_val = table(y = val_test$children, yhat = y_hat_val)
sum(diag(confusion_val))/sum(confusion_val)

roc_val <- roc(val_test$children, predicted_val)


auc <- round(auc(val_test$children, predicted_val), 4)

ggroc(roc_val) + 
  ggtitle(paste0('ROC Curve ', '(AUC = ', auc, ')'))
```

### Model Validation: Step 2

```{r K_fold, include = FALSE}
K_folds = 20
val_folds = createFolds(hotels_val$children, k = K_folds)

val_predict <- lapply(val_folds, function(x) {
  test = hotels_val[x,]
  pred = predict(lm_best, test, type = 'response')
  return(pred)
})

val_actual = lapply(val_folds, function(x) {
  test = hotels_val[x,]
  return(sum(test$children))
})

hotels_predicted = c()
hotels_difference = c()
for (k in seq(1, K_folds)) {
  hotels_predicted = append(hotels_predicted, as.integer(sum(unlist(val_predict[k]))))
  hotels_difference = append(hotels_difference, as.integer(unlist(val_actual[k])) - as.integer(hotels_predicted[k]))
}

hotels_final = cbind(hotels_predicted, val_actual, hotels_difference)
hotels_final = rbind(hotels_final, hotels_final %>% apply(2, unlist) %>% apply(2, abs) %>% apply(2, sum))

hotels_final
rownames(hotels_final)[21] = "Total"
hotels_final[21, 3] = as.integer(hotels_final[21, 1]) - as.integer(hotels_final[21, 2])

colnames(hotels_final) = c("Expected", "Actual", "Difference")

```

```{r echo = FALSE}
knitr::kable(hotels_final, caption = "Tabulated Performance of Best Model")
```

According to the table, over 20 folds of the `hotels_val` set there is only a difference of 15 between expected and actual number of bookings with children. With about 5,000 bookings in the 20 folds combined, this number is pretty low.
